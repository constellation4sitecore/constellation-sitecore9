<?xml version="1.0" encoding="utf-8" ?>
<configuration xmlns:patch="http://www.sitecore.net/xmlconfig/" xmlns:role="http://www.sitecore.net/xmlconfig/role/" >
	<sitecore>
		<!-- 
		
		
		
		DO NOT MODIFY THIS FILE. 
		
		USE SITECORE PATCH CONFIGS TO CHANGE BEHAVIOR. 
		
		ANY CHANGES TO THIS FILE MAY BE OVERWRITTEN IN A FUTURE RELEASE
		
		
		
		-->
		<constellation>
			<!-- 
			ROBOTS.TXT Handler Settings
			
			when called, the handler includes the following lines in the file:
			* a line indicating that all robots follow the same rules
			* a line pointing to the sitemap.xml document as the crawling source.
			* all Global Disallows (specified below)
			* all Site Disallows (example below)
			-->
			<robotsTxt>
				<!--
				GLOBAL RULES
				
				The allows and disallows below will be rendered in every robots.txt produced for every site in the system.
				We recommend the bare-minimum listed below to prevent your CMS login from showing up in 
				search results.
				-->
				<globalRules>
					<disallow>/sitecore</disallow>
					<disallow>/sitecore/</disallow>
				</globalRules>

				<!--
				DEFAULT RULES
				
				The allows and disallows below will be rendered in the robots.txt produced for every site that does not have
				its own custom rules. The recommended default is <allow>/</allow> which will allow crawling of a site from the root.
				To set the default to "do not crawl" change this to <disallow>/</disallow>. You can then specify which
				sites should be crawled individually. 
				
				-->
				<defaultRules>
					<!-- <allow>/</allow> -->
				</defaultRules>

				<!--
				SITE RULES

				You can customize the Allows and Disallows that are rendered in the robots.txt by site. Create a siteRules element as 
				shown below where the <siteName/> element is named after the sitecore <site/> element's name attribute value.
				
				NOTE THAT SETTING SITE RULES FOR A SITE WILL COMPLETELY OVERRIDE THE DEFAULT RULES
				
				You can't use both. However, Global rules will be included in every robots.txt response regardless of whether the site
				has unique rules.

				<siteRules>
					<siteName>
						<allow>/</allow>
						<disallow>/login</disallow>
					</siteName>
				</siteRules>
				
				Note that if you disallow the root of a site: <disallow>/</disallow> then the robots.txt will not include the sitemap.xml reference.
				-->
			</robotsTxt>
			<!-- 
			SITEMAP.XML Handler Settings
			
			SITES TO IGNORE
			
				Site names on this list will not return a Sitemap.Xml document and no document will be created for them.
				Default value includes all System sites.
				
				IMPORTANT: "website" is also on this list, because it is best practice to not use this site definition for production sites.
			
			CACHE ENABLED
			
				Stores the resulting XML document in the Sitecore cache for performance. Disable this for troubleshooting.
			
			DEFAULT CACHE TIMEOUT
				
				The cache timeout value can be defined for each site in the installation. the generated SITEMAP.XML will be cached for the duration specified
				for the site or the value below if not specified on the site.
				
				Default value is 240 minutes (4hrs)
				
			INCLUDE LAST MOD, INCLUDE CHANGE FREQUENCY, INCLUDE PRIORITY
			
				These settings are used to extend the simple XML format with additional search crawler information. Since Google and Bing both ignore
				these extended properties, these flags are set to false by default to keep the size of the XML document below 50MB.
			-->
			<sitemapXml
				sitesToIgnore="website,shell,login,admin,service,modules_shell,modules_website,scheduler,system,publisher,ucommerce"
				cacheEnabled="true"
				defaultCacheTimeout="240"
				includeLastMod="false"
				includeChangeFrequency="false"
				includePriority="false">
				<crawlers>
					<!-- 
					DEFAULT CRAWLERS
					
					You can supply one or more crawlers to create your sitemap.xml document. Each crawler will return ISitemapNodes that will be
					added to the sitemap.xml returned in the response. This allows you to provide specialized crawlers for handling wildcard or bucket
					areas of your site. All Crawlers must descend from Constellation.Foundation.SitemapXml.Crawlers.Crawler.
					
					It should also be noted that you can use a crawler to return an ISitemapNode that is not related to an Item, useful for creating
					e-commerce urls.
					
					It is in the programmer's interest to create unique site crawlers for the installation, if not on a project-by-project basis.
					The default crawler provided here will be used if a given site does not have its own crawler defined. SitemapXml ships with a 
					very basic crawler that starts at the context site's root node and walks down the tree one level at a time looking for Items
					that have presentation details. It stops when the maximum tree depth (set by Sitecore) is reached. This is a pretty inefficient
					crawler, and it does not have any considerations for hidden pages or no-index/no-follow logic. It will, however obey security, as its
					default database access is extranet\anonymous.
					-->
					<defaultCrawlers>
						<crawler type="Constellation.Foundation.SitemapXml.Crawlers.DefaultContentTreeCrawler, Constellation.Foundation.SitemapXml" />
					</defaultCrawlers>

					<!--
					SITE CRAWLERS
					
					If you need different sites to use different technology for producing sitemap.xml documents, you can override the defaults by specifying
					a set of crawlers for your site, by name. The following XML should be included in this sitecore/constellation/sitemapXml/crawlers node 
					of your configuration files.
					
					Example follows:
					
					<site name="example-site">
						<crawler type="ExampleNamespace.ExampleCustomCrawler, ExampleAssemblyName" />
						<crawler type="ExampleNamespace.AnotherCustomCrawler, ExampleAssemblyName" />
					</site>
					-->
				</crawlers>
			</sitemapXml>
		</constellation>

		<scheduling>
			<!--
			SITEMAP XML AGENT
			
			Hits the Sitemap XML repository to ensure there's always a cached copy of each site's Sitemap.Xml file in memory. We recommend running this at
			some fraction of the cache duration set for sitemaps above. The default is every 30 minutes, when the sitemaps are cached for 4hrs.
			
			If you have an aggressive background publish task, you are going to want to sync this task with your publishes so that the sitemaps are re-cached
			after the publish job. 
			
			WARNING: 
			This is a CPU-intensive task. Almost all Items in your content tree will be hit by this agent if caching is disabled! Watch server CPU performance 
			while these activities are going on and throttle back the frequency of sitemap generation, or disable this agent entirely if:
			
			* Sitemap generation takes longer than the duration between agent runs.
			* CPU performance on the delivery servers plateaus at 100% during a sitemap generation.
			-->
			<agent type="Constellation.Foundation.SitemapXml.Agents.SitemapXmlAgent" method="Refresh" interval="00:30:00" role:require="Standalone or ContentDelivery" />
		</scheduling>
		<events>
			<event name="publish:end">
				<!--
				REGENERATE SITEMAP XML
				
				After a publish run, re-crawls all sites to generate sitemap.xml documents, which are then cached. This can get expensive if publishes
				are frequent and may need to be disabled in high-volume edit environments.
				
				This handler is DISABLED by default to prevent sites that have a large number of Items (or integration with a 3rd party commerce system)
				from stalling manual publish processes.
				-->
				<!--
				<handler type="Constellation.Foundation.SitemapXml.EventHandlers.RegenerateSitemapXml" method="OnPublishEnd"/>
				-->
			</event>
		</events>
	</sitecore>
</configuration>